{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKscCJxRy7TI",
        "outputId": "254d2d10-621b-44d2-b024-d1d765eee993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting stix2\n",
            "  Downloading stix2-3.0.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from stix2) (2024.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stix2) (2.32.3)\n",
            "Collecting simplejson (from stix2)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting stix2-patterns>=1.2.0 (from stix2)\n",
            "  Downloading stix2_patterns-2.0.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting antlr4-python3-runtime~=4.9.0 (from stix2-patterns>=1.2.0->stix2)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from stix2-patterns>=1.2.0->stix2) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stix2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stix2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stix2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stix2) (2024.8.30)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stix2-3.0.1-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stix2_patterns-2.0.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k, antlr4-python3-runtime\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=ecc606f755e517e1ca0eb5e6d7dacc5fb9229546fb3bd956a78b4652c99e217c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=8b5a7ea392fbe5813147a1ff713c8cbe37cd9599db383297a0f469046f0a5e53\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built sgmllib3k antlr4-python3-runtime\n",
            "Installing collected packages: sgmllib3k, antlr4-python3-runtime, stix2-patterns, simplejson, feedparser, stix2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 feedparser-6.0.11 sgmllib3k-1.0.0 simplejson-3.19.3 stix2-3.0.1 stix2-patterns-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install feedparser stix2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import json\n",
        "import uuid\n",
        "from stix2 import Bundle, IntrusionSet\n",
        "from datetime import datetime, timezone\n",
        "import subprocess\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=\"rss_scraper_errors.log\",\n",
        "    level=logging.ERROR,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "# Proxy Configuration\n",
        "APP_PROXY = \"http://placeholder.com:85\"  # Replace with actual proxy address\n",
        "USERNAME = \"username_ui\"  # Replace with your username\n",
        "PASSWORD = \"password_ui\"  # Replace with your password\n",
        "PASSWORD = PASSWORD.replace(\"?\", \"\\\\?\")  # Escape special characters in the password\n",
        "\n",
        "\n",
        "def log_error(feed_url, message):\n",
        "    \"\"\"\n",
        "    Log an error message to the log file.\n",
        "    \"\"\"\n",
        "    logging.error(f\"Feed: {feed_url} - {message}\")\n",
        "\n",
        "\n",
        "def fetch_feed_content_with_proxy(feed_url):\n",
        "    \"\"\"\n",
        "    Fetch the content of an RSS feed using a proxy.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        str: The raw feed content or None if fetching fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Build the curl command with proxy and NTLM authentication\n",
        "        command = (\n",
        "            f'echo \"{PASSWORD}\" | '\n",
        "            f'curl -s -x \"{APP_PROXY}\" -U \"{USERNAME}\" \"{feed_url}\" --proxy-ntlm'\n",
        "        )\n",
        "        # Execute the command and get the response\n",
        "        response = subprocess.check_output(command, shell=True, text=True)\n",
        "        return response\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        log_error(feed_url, f\"Exception during fetch: {e}\")\n",
        "        print(f\"Error fetching feed: {feed_url}. Exception: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_feed(feed_content):\n",
        "    \"\"\"\n",
        "    Parse the content of an RSS feed.\n",
        "\n",
        "    Args:\n",
        "        feed_content (str): The raw feed content.\n",
        "\n",
        "    Returns:\n",
        "        dict: The parsed feed or None if parsing fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return feedparser.parse(feed_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing feed content. Exception: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def retry_feed_parsing(feed_url, retries=3):\n",
        "    \"\"\"\n",
        "    Retry fetching and parsing an RSS feed using a proxy.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "        retries (int): The number of retry attempts.\n",
        "\n",
        "    Returns:\n",
        "        dict: The parsed feed or None if all retries fail.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        print(f\"Attempt {attempt + 1} to fetch and parse feed: {feed_url}\")\n",
        "        feed_content = fetch_feed_content_with_proxy(feed_url)\n",
        "        if feed_content:\n",
        "            feed = parse_feed(feed_content)\n",
        "            if feed and not feed.bozo:  # Ensure the feed is valid\n",
        "                return feed\n",
        "        print(f\"Retrying feed: {feed_url}\")\n",
        "    log_error(feed_url, \"Failed to fetch or parse after retries.\")\n",
        "    print(f\"Failed to parse feed after {retries} attempts: {feed_url}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def scrape_rss_feed_to_stix(feed_url):\n",
        "    \"\"\"\n",
        "    Scrape an RSS feed and transform the data into STIX 2.1 JSON format.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        str: JSON string in STIX format or None if parsing fails.\n",
        "    \"\"\"\n",
        "    feed = retry_feed_parsing(feed_url)\n",
        "    if not feed:\n",
        "        return None\n",
        "\n",
        "    # Collect STIX objects\n",
        "    stix_objects = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        intrusion_set_id = f\"intrusion-set--{uuid.uuid4()}\"\n",
        "\n",
        "        # Parse published date\n",
        "        published_date = entry.get(\"published\", None)\n",
        "        if published_date:\n",
        "            try:\n",
        "                published_date = datetime.strptime(published_date, \"%a, %d %b %Y %H:%M:%S %z\")\n",
        "            except ValueError:\n",
        "                try:\n",
        "                    published_date = datetime.strptime(published_date, \"%a, %d %b %Y %H:%M:%S +0000\")\n",
        "                except ValueError:\n",
        "                    print(f\"Failed to parse date for entry: {entry.title}\")\n",
        "                    continue\n",
        "\n",
        "            published_date = published_date.astimezone(timezone.utc)\n",
        "\n",
        "            intrusion_set = IntrusionSet(\n",
        "                id=intrusion_set_id,\n",
        "                name=entry.title,\n",
        "                description=entry.get(\"summary\", \"No description available.\"),\n",
        "                first_seen=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                created=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                modified=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                resource_level=\"unknown\",\n",
        "                primary_motivation=\"unknown\",\n",
        "                aliases=[]\n",
        "            )\n",
        "            stix_objects.append(intrusion_set)\n",
        "\n",
        "    # Create a STIX Bundle\n",
        "    stix_bundle = Bundle(objects=stix_objects)\n",
        "    return stix_bundle.serialize(pretty=True)\n",
        "\n",
        "\n",
        "def process_multiple_feeds(feed_urls):\n",
        "    \"\"\"\n",
        "    Process multiple RSS feeds and save STIX bundles for each feed.\n",
        "\n",
        "    Args:\n",
        "        feed_urls (list): List of RSS feed URLs.\n",
        "    \"\"\"\n",
        "    for feed_url in feed_urls:\n",
        "        print(f\"Processing feed: {feed_url}\")\n",
        "        stix_json = scrape_rss_feed_to_stix(feed_url)\n",
        "        if stix_json:\n",
        "            domain = feed_url.split(\"//\")[-1].split(\"/\")[0]\n",
        "            filename = f\"stix_bundle_{domain}.json\"\n",
        "            with open(filename, \"w\") as json_file:\n",
        "                json_file.write(stix_json)\n",
        "            print(f\"STIX bundle saved to '{filename}'.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # List of RSS feed URLs\n",
        "    rss_feed_urls = [\n",
        "        \"https://feeds.feedburner.com/TheHackersNews?format=xml\",\n",
        "        \"https://www.wired.com/feed/category/security/latest/rss\",\n",
        "        \"https://www.bleepingcomputer.com/feed/\"\n",
        "    ]\n",
        "\n",
        "    process_multiple_feeds(rss_feed_urls)\n",
        "\n"
      ],
      "metadata": {
        "id": "OfXfWR-abRA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import json\n",
        "import uuid\n",
        "from stix2 import Bundle, IntrusionSet\n",
        "from datetime import datetime, timezone\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=\"rss_scraper_errors.log\",\n",
        "    level=logging.ERROR,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "def log_error(feed_url, message):\n",
        "    \"\"\"\n",
        "    Log an error message to the log file.\n",
        "    \"\"\"\n",
        "    logging.error(f\"Feed: {feed_url} - {message}\")\n",
        "\n",
        "def fetch_feed_content(feed_url):\n",
        "    \"\"\"\n",
        "    Fetch the content of an RSS feed.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        str: The raw feed content or None if fetching fails.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(feed_url, headers=headers)\n",
        "        response.encoding = 'utf-8'  # Ensure proper encoding\n",
        "        if response.status_code != 200:\n",
        "            log_error(feed_url, f\"HTTP Status Code: {response.status_code}\")\n",
        "            print(f\"Failed to fetch feed: {feed_url}. HTTP Status: {response.status_code}\")\n",
        "            return None\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        log_error(feed_url, f\"Exception during fetch: {e}\")\n",
        "        print(f\"Error fetching feed: {feed_url}. Exception: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_feed(feed_content):\n",
        "    \"\"\"\n",
        "    Parse the content of an RSS feed.\n",
        "\n",
        "    Args:\n",
        "        feed_content (str): The raw feed content.\n",
        "\n",
        "    Returns:\n",
        "        dict: The parsed feed or None if parsing fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return feedparser.parse(feed_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing feed content. Exception: {e}\")\n",
        "        return None\n",
        "\n",
        "def retry_feed_parsing(feed_url, retries=3):\n",
        "    \"\"\"\n",
        "    Retry fetching and parsing an RSS feed.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "        retries (int): The number of retry attempts.\n",
        "\n",
        "    Returns:\n",
        "        dict: The parsed feed or None if all retries fail.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        print(f\"Attempt {attempt + 1} to fetch and parse feed: {feed_url}\")\n",
        "        feed_content = fetch_feed_content(feed_url)\n",
        "        if feed_content:\n",
        "            feed = parse_feed(feed_content)\n",
        "            if feed and not feed.bozo:  # Ensure the feed is valid\n",
        "                return feed\n",
        "        print(f\"Retrying feed: {feed_url}\")\n",
        "    log_error(feed_url, \"Failed to fetch or parse after retries.\")\n",
        "    print(f\"Failed to parse feed after {retries} attempts: {feed_url}\")\n",
        "    return None\n",
        "\n",
        "def scrape_rss_feed_to_stix(feed_url):\n",
        "    \"\"\"\n",
        "    Scrape an RSS feed and transform the data into STIX 2.1 JSON format.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        str: JSON string in STIX format or None if parsing fails.\n",
        "    \"\"\"\n",
        "    feed = retry_feed_parsing(feed_url)\n",
        "    if not feed:\n",
        "        return None\n",
        "\n",
        "    # Collect STIX objects\n",
        "    stix_objects = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        intrusion_set_id = f\"intrusion-set--{uuid.uuid4()}\"\n",
        "\n",
        "        # Parse published date\n",
        "        published_date = entry.get(\"published\", None)\n",
        "        if published_date:\n",
        "            try:\n",
        "                published_date = datetime.strptime(published_date, \"%a, %d %b %Y %H:%M:%S %z\")\n",
        "            except ValueError:\n",
        "                try:\n",
        "                    published_date = datetime.strptime(published_date, \"%a, %d %b %Y %H:%M:%S +0000\")\n",
        "                except ValueError:\n",
        "                    print(f\"Failed to parse date for entry: {entry.title}\")\n",
        "                    continue\n",
        "\n",
        "            published_date = published_date.astimezone(timezone.utc)\n",
        "\n",
        "            intrusion_set = IntrusionSet(\n",
        "                id=intrusion_set_id,\n",
        "                name=entry.title,\n",
        "                description=entry.get(\"summary\", \"No description available.\"),\n",
        "                first_seen=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                created=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                modified=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                resource_level=\"unknown\",\n",
        "                primary_motivation=\"unknown\",\n",
        "                aliases=[]\n",
        "            )\n",
        "            stix_objects.append(intrusion_set)\n",
        "\n",
        "    # Create a STIX Bundle\n",
        "    stix_bundle = Bundle(objects=stix_objects)\n",
        "    return stix_bundle.serialize(pretty=True)\n",
        "\n",
        "def process_multiple_feeds(feed_urls):\n",
        "    \"\"\"\n",
        "    Process multiple RSS feeds and save STIX bundles for each feed.\n",
        "\n",
        "    Args:\n",
        "        feed_urls (list): List of RSS feed URLs.\n",
        "    \"\"\"\n",
        "    for feed_url in feed_urls:\n",
        "        print(f\"Processing feed: {feed_url}\")\n",
        "        stix_json = scrape_rss_feed_to_stix(feed_url)\n",
        "        if stix_json:\n",
        "            domain = feed_url.split(\"//\")[-1].split(\"/\")[0]\n",
        "            filename = f\"stix_bundle_{domain}.json\"\n",
        "            with open(filename, \"w\") as json_file:\n",
        "                json_file.write(stix_json)\n",
        "            print(f\"STIX bundle saved to '{filename}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # List of RSS feed URLs\n",
        "    rss_feed_urls = [\n",
        "        \"https://feeds.feedburner.com/TheHackersNews?format=xml\",\n",
        "        \"https://www.wired.com/feed/category/security/latest/rss\",\n",
        "        \"https://www.bleepingcomputer.com/feed/\"\n",
        "    ]\n",
        "\n",
        "    process_multiple_feeds(rss_feed_urls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ACOOock6D_O",
        "outputId": "27ed8983-9f6a-49a6-ba9b-679180af7da2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing feed: https://feeds.feedburner.com/TheHackersNews?format=xml\n",
            "Attempt 1 to fetch and parse feed: https://feeds.feedburner.com/TheHackersNews?format=xml\n",
            "STIX bundle saved to 'stix_bundle_feeds.feedburner.com.json'.\n",
            "Processing feed: https://www.wired.com/feed/category/security/latest/rss\n",
            "Attempt 1 to fetch and parse feed: https://www.wired.com/feed/category/security/latest/rss\n",
            "STIX bundle saved to 'stix_bundle_www.wired.com.json'.\n",
            "Processing feed: https://www.bleepingcomputer.com/feed/\n",
            "Attempt 1 to fetch and parse feed: https://www.bleepingcomputer.com/feed/\n",
            "STIX bundle saved to 'stix_bundle_www.bleepingcomputer.com.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ficCX8lu7Me1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rss_scraper/\n",
        "├── __init__.py\n",
        "├── config.py             \n",
        "├── main.py               \n",
        "├── scraper.py            \n",
        "├── stix_converter.py     \n",
        "├── tests/                \n",
        "│   ├── __init__.py\n",
        "│   ├── test_scraper.py   \n",
        "│   ├── test_stix_converter.py  \n",
        "├── logs/                 \n",
        "│   ├── rss_scraper_errors.log\n",
        "├── stix_bundles/         \n",
        "├── requirements.txt      \n",
        "├── setup.py              \n"
      ],
      "metadata": {
        "id": "1QNdyv_8G8KA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBFCLvNvtpsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retry_feed_with_backoff(feed_url, retries=3, delay=5):\n",
        "    \"\"\"\n",
        "    Retry fetching the feed with exponential backoff.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "        retries (int): Maximum number of retry attempts.\n",
        "        delay (int): Initial delay in seconds before retrying.\n",
        "\n",
        "    Returns:\n",
        "        str: The feed content if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    for attempt in range(1, retries + 1):\n",
        "        print(f\"Attempt {attempt} to fetch feed: {feed_url}\")\n",
        "        response = fetch_feed_content_with_proxy(feed_url)\n",
        "        if response:\n",
        "            return response\n",
        "        print(f\"Retrying in {delay} seconds...\")\n",
        "        time.sleep(delay)\n",
        "        delay *= 2  # Exponential backoff\n",
        "    print(f\"Failed to fetch feed after {retries} attempts: {feed_url}\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "LiLz3WwRtpxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retry_feed_parsing(feed_url, retries=3):\n",
        "    \"\"\"\n",
        "    Retry fetching and parsing an RSS feed using exponential backoff.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "        retries (int): Maximum number of retry attempts.\n",
        "\n",
        "    Returns:\n",
        "        dict: The parsed feed or None if all retries fail.\n",
        "    \"\"\"\n",
        "    feed_content = retry_feed_with_backoff(feed_url, retries=retries)\n",
        "    if feed_content:\n",
        "        feed = parse_feed(feed_content)\n",
        "        if feed and not feed.bozo:  # Ensure the feed is valid\n",
        "            return feed\n",
        "    log_error(feed_url, \"Failed to fetch or parse after retries.\")\n",
        "    print(f\"Failed to parse feed after {retries} attempts: {feed_url}\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "OyrklHeJtws0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
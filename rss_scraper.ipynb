{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKscCJxRy7TI",
        "outputId": "254d2d10-621b-44d2-b024-d1d765eee993"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting stix2\n",
            "  Downloading stix2-3.0.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from stix2) (2024.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stix2) (2.32.3)\n",
            "Collecting simplejson (from stix2)\n",
            "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting stix2-patterns>=1.2.0 (from stix2)\n",
            "  Downloading stix2_patterns-2.0.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting antlr4-python3-runtime~=4.9.0 (from stix2-patterns>=1.2.0->stix2)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from stix2-patterns>=1.2.0->stix2) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stix2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stix2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stix2) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stix2) (2024.8.30)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stix2-3.0.1-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stix2_patterns-2.0.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k, antlr4-python3-runtime\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=ecc606f755e517e1ca0eb5e6d7dacc5fb9229546fb3bd956a78b4652c99e217c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=8b5a7ea392fbe5813147a1ff713c8cbe37cd9599db383297a0f469046f0a5e53\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built sgmllib3k antlr4-python3-runtime\n",
            "Installing collected packages: sgmllib3k, antlr4-python3-runtime, stix2-patterns, simplejson, feedparser, stix2\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 feedparser-6.0.11 sgmllib3k-1.0.0 simplejson-3.19.3 stix2-3.0.1 stix2-patterns-2.0.0\n"
          ]
        }
      ],
      "source": [
        "pip install feedparser stix2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import feedparser\n",
        "import json\n",
        "import uuid\n",
        "from stix2 import Bundle, IntrusionSet\n",
        "from datetime import datetime, timezone\n",
        "import requests\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=\"rss_scraper_errors.log\",\n",
        "    level=logging.ERROR,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "def log_error(feed_url, message):\n",
        "    \"\"\"\n",
        "    Log an error message to the log file.\n",
        "    \"\"\"\n",
        "    logging.error(f\"Feed: {feed_url} - {message}\")\n",
        "\n",
        "def fetch_feed_content(feed_url):\n",
        "    \"\"\"\n",
        "    Fetch the content of an RSS feed.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        str: The raw feed content or None if fetching fails.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(feed_url, headers=headers)\n",
        "        response.encoding = 'utf-8'  # Ensure proper encoding\n",
        "        if response.status_code != 200:\n",
        "            log_error(feed_url, f\"HTTP Status Code: {response.status_code}\")\n",
        "            print(f\"Failed to fetch feed: {feed_url}. HTTP Status: {response.status_code}\")\n",
        "            return None\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        log_error(feed_url, f\"Exception during fetch: {e}\")\n",
        "        print(f\"Error fetching feed: {feed_url}. Exception: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_feed(feed_content):\n",
        "    \"\"\"\n",
        "    Parse the content of an RSS feed.\n",
        "\n",
        "    Args:\n",
        "        feed_content (str): The raw feed content.\n",
        "\n",
        "    Returns:\n",
        "        dict: The parsed feed or None if parsing fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return feedparser.parse(feed_content)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing feed content. Exception: {e}\")\n",
        "        return None\n",
        "\n",
        "def retry_feed_parsing(feed_url, retries=3):\n",
        "    \"\"\"\n",
        "    Retry fetching and parsing an RSS feed.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "        retries (int): The number of retry attempts.\n",
        "\n",
        "    Returns:\n",
        "        dict: The parsed feed or None if all retries fail.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        print(f\"Attempt {attempt + 1} to fetch and parse feed: {feed_url}\")\n",
        "        feed_content = fetch_feed_content(feed_url)\n",
        "        if feed_content:\n",
        "            feed = parse_feed(feed_content)\n",
        "            if feed and not feed.bozo:  # Ensure the feed is valid\n",
        "                return feed\n",
        "        print(f\"Retrying feed: {feed_url}\")\n",
        "    log_error(feed_url, \"Failed to fetch or parse after retries.\")\n",
        "    print(f\"Failed to parse feed after {retries} attempts: {feed_url}\")\n",
        "    return None\n",
        "\n",
        "def scrape_rss_feed_to_stix(feed_url):\n",
        "    \"\"\"\n",
        "    Scrape an RSS feed and transform the data into STIX 2.1 JSON format.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        str: JSON string in STIX format or None if parsing fails.\n",
        "    \"\"\"\n",
        "    feed = retry_feed_parsing(feed_url)\n",
        "    if not feed:\n",
        "        return None\n",
        "\n",
        "    # Collect STIX objects\n",
        "    stix_objects = []\n",
        "\n",
        "    for entry in feed.entries:\n",
        "        intrusion_set_id = f\"intrusion-set--{uuid.uuid4()}\"\n",
        "\n",
        "        # Parse published date\n",
        "        published_date = entry.get(\"published\", None)\n",
        "        if published_date:\n",
        "            try:\n",
        "                published_date = datetime.strptime(published_date, \"%a, %d %b %Y %H:%M:%S %z\")\n",
        "            except ValueError:\n",
        "                try:\n",
        "                    published_date = datetime.strptime(published_date, \"%a, %d %b %Y %H:%M:%S +0000\")\n",
        "                except ValueError:\n",
        "                    print(f\"Failed to parse date for entry: {entry.title}\")\n",
        "                    continue\n",
        "\n",
        "            published_date = published_date.astimezone(timezone.utc)\n",
        "\n",
        "            intrusion_set = IntrusionSet(\n",
        "                id=intrusion_set_id,\n",
        "                name=entry.title,\n",
        "                description=entry.get(\"summary\", \"No description available.\"),\n",
        "                first_seen=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                created=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                modified=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "                resource_level=\"unknown\",\n",
        "                primary_motivation=\"unknown\",\n",
        "                aliases=[]\n",
        "            )\n",
        "            stix_objects.append(intrusion_set)\n",
        "\n",
        "    # Create a STIX Bundle\n",
        "    stix_bundle = Bundle(objects=stix_objects)\n",
        "    return stix_bundle.serialize(pretty=True)\n",
        "\n",
        "def process_multiple_feeds(feed_urls):\n",
        "    \"\"\"\n",
        "    Process multiple RSS feeds and save STIX bundles for each feed.\n",
        "\n",
        "    Args:\n",
        "        feed_urls (list): List of RSS feed URLs.\n",
        "    \"\"\"\n",
        "    for feed_url in feed_urls:\n",
        "        print(f\"Processing feed: {feed_url}\")\n",
        "        stix_json = scrape_rss_feed_to_stix(feed_url)\n",
        "        if stix_json:\n",
        "            domain = feed_url.split(\"//\")[-1].split(\"/\")[0]\n",
        "            filename = f\"stix_bundle_{domain}.json\"\n",
        "            with open(filename, \"w\") as json_file:\n",
        "                json_file.write(stix_json)\n",
        "            print(f\"STIX bundle saved to '{filename}'.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # List of RSS feed URLs\n",
        "    rss_feed_urls = [\n",
        "        \"https://feeds.feedburner.com/TheHackersNews?format=xml\",\n",
        "        \"https://www.wired.com/feed/category/security/latest/rss\",\n",
        "        \"https://www.bleepingcomputer.com/feed/\"\n",
        "    ]\n",
        "\n",
        "    process_multiple_feeds(rss_feed_urls)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ACOOock6D_O",
        "outputId": "27ed8983-9f6a-49a6-ba9b-679180af7da2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing feed: https://feeds.feedburner.com/TheHackersNews?format=xml\n",
            "Attempt 1 to fetch and parse feed: https://feeds.feedburner.com/TheHackersNews?format=xml\n",
            "STIX bundle saved to 'stix_bundle_feeds.feedburner.com.json'.\n",
            "Processing feed: https://www.wired.com/feed/category/security/latest/rss\n",
            "Attempt 1 to fetch and parse feed: https://www.wired.com/feed/category/security/latest/rss\n",
            "STIX bundle saved to 'stix_bundle_www.wired.com.json'.\n",
            "Processing feed: https://www.bleepingcomputer.com/feed/\n",
            "Attempt 1 to fetch and parse feed: https://www.bleepingcomputer.com/feed/\n",
            "STIX bundle saved to 'stix_bundle_www.bleepingcomputer.com.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ficCX8lu7Me1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rss_scraper/\n",
        "├── __init__.py\n",
        "├── config.py             \n",
        "├── main.py               \n",
        "├── scraper.py            \n",
        "├── stix_converter.py     \n",
        "├── tests/                \n",
        "│   ├── __init__.py\n",
        "│   ├── test_scraper.py   \n",
        "│   ├── test_stix_converter.py  \n",
        "├── logs/                 \n",
        "│   ├── rss_scraper_errors.log\n",
        "├── stix_bundles/         \n",
        "├── requirements.txt      \n",
        "├── setup.py              \n"
      ],
      "metadata": {
        "id": "1QNdyv_8G8KA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scraper.py:\n",
        "\n",
        "#Handles all RSS-related functionality, including fetching, parsing, and retry logic."
      ],
      "metadata": {
        "id": "Tf4xgvZEG_4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import feedparser\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=\"logs/rss_scraper_errors.log\",\n",
        "    level=logging.ERROR,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
        ")\n",
        "\n",
        "def fetch_feed_content(feed_url):\n",
        "    \"\"\"Fetch the raw content of an RSS feed.\n",
        "\n",
        "    Makes an HTTP GET request to the given RSS feed URL and retrieves the content as text.\n",
        "    Handles HTTP status codes and logs errors for failed requests.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        str: The raw feed content as a string if the request is successful.\n",
        "        None: If the request fails or an exception occurs.\n",
        "    \"\"\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
        "    try:\n",
        "        response = requests.get(feed_url, headers=headers)\n",
        "        response.encoding = 'utf-8'\n",
        "        if response.status_code != 200:\n",
        "            logging.error(f\"Feed: {feed_url} - HTTP Status Code: {response.status_code}\")\n",
        "            return None\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Feed: {feed_url} - Exception: {e}\")\n",
        "        return None\n",
        "\n",
        "def parse_feed(feed_content):\n",
        "    \"\"\"Parse RSS feed content into a structured format.\n",
        "\n",
        "    Uses the `feedparser` library to parse the raw RSS feed content into a structured object.\n",
        "\n",
        "    Args:\n",
        "        feed_content (str): The raw feed content as a string.\n",
        "\n",
        "    Returns:\n",
        "        feedparser.FeedParserDict: A structured object representing the parsed RSS feed.\n",
        "        None: If parsing fails or an exception occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return feedparser.parse(feed_content)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Exception parsing feed content: {e}\")\n",
        "        return None\n",
        "\n",
        "def retry_feed_parsing(feed_url, retries=3):\n",
        "    \"\"\"Retry fetching and parsing an RSS feed multiple times.\n",
        "\n",
        "    Attempts to fetch and parse the RSS feed content up to the specified number of retries.\n",
        "    Logs each failed attempt and returns the parsed feed on success.\n",
        "\n",
        "    Args:\n",
        "        feed_url (str): The URL of the RSS feed.\n",
        "        retries (int, optional): The maximum number of retry attempts. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        feedparser.FeedParserDict: A structured object representing the parsed RSS feed on success.\n",
        "        None: If all retries fail.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        feed_content = fetch_feed_content(feed_url)\n",
        "        if feed_content:\n",
        "            feed = parse_feed(feed_content)\n",
        "            if feed and not feed.bozo:  # Ensure the feed is valid\n",
        "                return feed\n",
        "        logging.error(f\"Retry {attempt + 1} failed for feed: {feed_url}\")\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "kJVCvpbnHW8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stix_converter.py:\n",
        "\n",
        "#Handles conversion of RSS data into STIX format."
      ],
      "metadata": {
        "id": "tZNFf18xHZXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from datetime import datetime, timezone\n",
        "from stix2 import Bundle, IntrusionSet\n",
        "\n",
        "def convert_entry_to_stix(entry):\n",
        "    \"\"\"Convert an RSS feed entry to a STIX IntrusionSet object.\n",
        "\n",
        "    Parses the provided RSS feed entry and converts it into a STIX `IntrusionSet` object.\n",
        "    The function generates a unique identifier and processes the published date.\n",
        "\n",
        "    Args:\n",
        "        entry (dict): A dictionary representing an RSS feed entry, typically containing\n",
        "                      fields like 'title', 'summary', and 'published'.\n",
        "\n",
        "    Returns:\n",
        "        stix2.IntrusionSet: A STIX IntrusionSet object containing the converted entry data.\n",
        "        None: If the entry does not contain a valid published date.\n",
        "    \"\"\"\n",
        "    intrusion_set_id = f\"intrusion-set--{uuid.uuid4()}\"\n",
        "    published_date = entry.get(\"published\", None)\n",
        "\n",
        "    if published_date:\n",
        "        try:\n",
        "            published_date = datetime.strptime(published_date, \"%a, %d %b %Y %H:%M:%S %z\")\n",
        "        except ValueError:\n",
        "            return None\n",
        "        published_date = published_date.astimezone(timezone.utc)\n",
        "\n",
        "    return IntrusionSet(\n",
        "        id=intrusion_set_id,\n",
        "        name=entry.get(\"title\", \"Unknown Title\"),\n",
        "        description=entry.get(\"summary\", \"No description available.\"),\n",
        "        first_seen=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "        created=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "        modified=published_date.isoformat().replace(\"+00:00\", \"Z\"),\n",
        "        resource_level=\"unknown\",\n",
        "        primary_motivation=\"unknown\",\n",
        "        aliases=[]\n",
        "    )\n",
        "\n",
        "def convert_feed_to_stix(feed):\n",
        "    \"\"\"Convert an entire RSS feed into a STIX Bundle.\n",
        "\n",
        "    Processes all entries in the provided RSS feed and converts them into STIX `IntrusionSet` objects.\n",
        "    Combines all the converted objects into a single STIX `Bundle`.\n",
        "\n",
        "    Args:\n",
        "        feed (feedparser.FeedParserDict): A parsed RSS feed object, typically containing\n",
        "                                          a list of entries in `feed.entries`.\n",
        "\n",
        "    Returns:\n",
        "        str: A serialized STIX Bundle containing all the converted entries in JSON format.\n",
        "    \"\"\"\n",
        "    stix_objects = [\n",
        "        convert_entry_to_stix(entry) for entry in feed.entries if entry\n",
        "    ]\n",
        "    return Bundle(objects=stix_objects).serialize(pretty=True)\n"
      ],
      "metadata": {
        "id": "LU-HkX-BHvVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# config.py:\n",
        "\n",
        "#Stores program configurations such as RSS feed URLs and retry settings."
      ],
      "metadata": {
        "id": "BC29TCqfHypb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration for RSS feeds\n",
        "RSS_FEEDS = [\n",
        "    \"https://feeds.feedburner.com/TheHackersNews?format=xml\",\n",
        "    \"https://www.wired.com/feed/category/security/latest/rss\",\n",
        "    \"https://www.bleepingcomputer.com/feed/\"\n",
        "]\n",
        "\n",
        "# Retry settings\n",
        "RETRY_COUNT = 3\n"
      ],
      "metadata": {
        "id": "j2KKwmMqIKBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py:\n",
        "\n",
        "#The entry point for the program that coordinates fetching, parsing, and saving data."
      ],
      "metadata": {
        "id": "wF2GMC2kIQn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from scraper import retry_feed_parsing\n",
        "from stix_converter import convert_feed_to_stix\n",
        "from config import RSS_FEEDS, RETRY_COUNT\n",
        "\n",
        "def save_stix_to_file(stix_json, filename):\n",
        "    \"\"\"Save a STIX JSON string to a file.\n",
        "\n",
        "    Creates the output directory if it does not exist, and writes the STIX JSON\n",
        "    string to a specified file.\n",
        "\n",
        "    Args:\n",
        "        stix_json (str): The serialized STIX JSON string to save.\n",
        "        filename (str): The name of the file to save the JSON content.\n",
        "\n",
        "    Side Effects:\n",
        "        Creates the directory `stix_bundles` if it doesn't exist.\n",
        "        Writes the STIX JSON string to the specified file.\n",
        "\n",
        "    Example:\n",
        "        >>> save_stix_to_file(stix_json, \"stix_bundle_example.json\")\n",
        "        STIX bundle saved to stix_bundles/stix_bundle_example.json\n",
        "    \"\"\"\n",
        "    output_dir = \"stix_bundles\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    with open(filepath, \"w\") as f:\n",
        "        f.write(stix_json)\n",
        "    print(f\"STIX bundle saved to {filepath}\")\n",
        "\n",
        "def process_feeds():\n",
        "    \"\"\"Process a list of RSS feeds, convert them to STIX bundles, and save to files.\n",
        "\n",
        "    Iterates through the RSS feed URLs defined in the `RSS_FEEDS` configuration,\n",
        "    attempts to parse each feed, converts the feed content to a STIX bundle, and\n",
        "    saves the bundle to a file.\n",
        "\n",
        "    Uses the `retry_feed_parsing` function to handle retry logic for fetching and\n",
        "    parsing feeds. The STIX bundles are saved using the `save_stix_to_file` function.\n",
        "\n",
        "    Side Effects:\n",
        "        Writes STIX JSON bundles to files in the `stix_bundles` directory.\n",
        "\n",
        "    Example:\n",
        "        If `RSS_FEEDS` contains the following URLs:\n",
        "        - \"https://feeds.feedburner.com/TheHackersNews?format=xml\"\n",
        "        - \"https://www.bleepingcomputer.com/feed/\"\n",
        "\n",
        "        The corresponding files will be saved as:\n",
        "        - `stix_bundles/stix_bundle_feeds.feedburner.com.json`\n",
        "        - `stix_bundles/stix_bundle_www.bleepingcomputer.com.json`\n",
        "    \"\"\"\n",
        "    for feed_url in RSS_FEEDS:\n",
        "        print(f\"Processing feed: {feed_url}\")\n",
        "        feed = retry_feed_parsing(feed_url, RETRY_COUNT)\n",
        "        if feed:\n",
        "            stix_json = convert_feed_to_stix(feed)\n",
        "            domain = feed_url.split(\"//\")[-1].split(\"/\")[0]\n",
        "            save_stix_to_file(stix_json, f\"stix_bundle_{domain}.json\")\n",
        "        else:\n",
        "            print(f\"Failed to process feed: {feed_url}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_feeds()"
      ],
      "metadata": {
        "id": "AoilysNoIVIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a setup.py file for packaging."
      ],
      "metadata": {
        "id": "GgUqEmGzIXgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(\n",
        "    name=\"rss_scraper\",\n",
        "    version=\"1.0.0\",\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        \"feedparser\",\n",
        "        \"stix2\",\n",
        "        \"requests\",\n",
        "    ],\n",
        "    entry_points={\n",
        "        \"console_scripts\": [\n",
        "            \"rss-scraper=main:process_feeds\",\n",
        "        ]\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "U4M_ttSoIvVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for parse_feed\n",
        "\n",
        "#This test ensures that the parse_feed function correctly parses valid RSS feed\n",
        "#content and handles invalid content gracefully.\n",
        "\n",
        "#tests/test_scraper.py"
      ],
      "metadata": {
        "id": "_QkoB-3iIymw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from scraper import parse_feed\n",
        "\n",
        "class TestParseFeed(unittest.TestCase):\n",
        "    \"\"\"Unit tests for the `parse_feed` function.\n",
        "\n",
        "    This test suite verifies that the `parse_feed` function correctly parses valid\n",
        "    RSS feed content into a structured object and handles invalid content gracefully.\n",
        "    \"\"\"\n",
        "\n",
        "    def test_parse_valid_feed(self):\n",
        "        \"\"\"Test parsing of a valid RSS feed.\n",
        "\n",
        "        Verifies that `parse_feed` successfully parses a well-formed RSS feed string\n",
        "        and extracts the correct feed and entry information.\n",
        "\n",
        "        Example:\n",
        "            Feed Content:\n",
        "            - Title: \"Example Feed\"\n",
        "            - Entry Title: \"Test Entry\"\n",
        "            - Entry Link: \"https://example.com/test-entry\"\n",
        "            - Entry Description: \"Test Description\"\n",
        "            - Entry Publication Date: \"Mon, 01 Jan 2024 10:00:00 +0000\"\n",
        "\n",
        "        Assertions:\n",
        "            - The parsed feed object is not `None`.\n",
        "            - The feed's title matches the expected value.\n",
        "            - The number of entries matches the expected value.\n",
        "            - The first entry's title matches the expected value.\n",
        "        \"\"\"\n",
        "        valid_feed_content = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n",
        "        <rss version=\"2.0\">\n",
        "            <channel>\n",
        "                <title>Example Feed</title>\n",
        "                <item>\n",
        "                    <title>Test Entry</title>\n",
        "                    <link>https://example.com/test-entry</link>\n",
        "                    <description>Test Description</description>\n",
        "                    <pubDate>Mon, 01 Jan 2024 10:00:00 +0000</pubDate>\n",
        "                </item>\n",
        "            </channel>\n",
        "        </rss>\"\"\"\n",
        "        parsed_feed = parse_feed(valid_feed_content)\n",
        "        self.assertIsNotNone(parsed_feed)\n",
        "        self.assertEqual(parsed_feed.feed.title, \"Example Feed\")\n",
        "        self.assertEqual(len(parsed_feed.entries), 1)\n",
        "        self.assertEqual(parsed_feed.entries[0].title, \"Test Entry\")\n",
        "\n",
        "    def test_parse_invalid_feed(self):\n",
        "        \"\"\"Test parsing of an invalid RSS feed.\n",
        "\n",
        "        Verifies that `parse_feed` correctly identifies and handles invalid RSS feed\n",
        "        content by returning `None`.\n",
        "\n",
        "        Example:\n",
        "            Feed Content:\n",
        "            - \"<rss><invalid></invalid></rss>\"\n",
        "\n",
        "        Assertions:\n",
        "            - The parsed feed object is `None`.\n",
        "        \"\"\"\n",
        "        invalid_feed_content = \"<rss><invalid></invalid></rss>\"\n",
        "        parsed_feed = parse_feed(invalid_feed_content)\n",
        "        self.assertIsNone(parsed_feed)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n",
        "\n"
      ],
      "metadata": {
        "id": "csFF-c7QI_ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for convert_entry_to_stix\n",
        "\n",
        "#This test ensures that convert_entry_to_stix correctly converts RSS entries\n",
        "#into STIX objects.\n",
        "\n",
        "#File: tests/test_stix_converter.py"
      ],
      "metadata": {
        "id": "nOjTBObrJCGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "from stix_converter import convert_entry_to_stix\n",
        "\n",
        "class TestConvertEntryToStix(unittest.TestCase):\n",
        "    \"\"\"Unit tests for the `convert_entry_to_stix` function.\n",
        "\n",
        "    This test suite verifies that the `convert_entry_to_stix` function correctly\n",
        "    converts RSS feed entries into STIX IntrusionSet objects and handles edge cases\n",
        "    such as missing required fields.\n",
        "    \"\"\"\n",
        "\n",
        "    def test_convert_valid_entry(self):\n",
        "        \"\"\"Test conversion of a valid RSS entry to a STIX object.\n",
        "\n",
        "        Verifies that a properly formatted RSS entry is converted into a valid STIX\n",
        "        IntrusionSet object with correctly mapped fields.\n",
        "\n",
        "        Example:\n",
        "            Entry:\n",
        "            - Title: \"Test Entry\"\n",
        "            - Summary: \"This is a test entry.\"\n",
        "            - Published: \"Mon, 01 Jan 2024 10:00:00 +0000\"\n",
        "            - Link: \"https://example.com/test-entry\"\n",
        "\n",
        "        Assertions:\n",
        "            - The returned STIX object is not `None`.\n",
        "            - The object's name matches the entry's title.\n",
        "            - The object's description matches the entry's summary.\n",
        "            - The object's `first_seen` timestamp is in ISO 8601 format with a \"Z\" suffix.\n",
        "        \"\"\"\n",
        "        entry = {\n",
        "            \"title\": \"Test Entry\",\n",
        "            \"summary\": \"This is a test entry.\",\n",
        "            \"published\": \"Mon, 01 Jan 2024 10:00:00 +0000\",\n",
        "            \"link\": \"https://example.com/test-entry\",\n",
        "        }\n",
        "        stix_object = convert_entry_to_stix(entry)\n",
        "        self.assertIsNotNone(stix_object)\n",
        "        self.assertEqual(stix_object.name, \"Test Entry\")\n",
        "        self.assertEqual(stix_object.description, \"This is a test entry.\")\n",
        "        self.assertTrue(stix_object.first_seen.endswith(\"Z\"))\n",
        "\n",
        "    def test_convert_entry_missing_published_date(self):\n",
        "        \"\"\"Test conversion of an RSS entry missing a published date.\n",
        "\n",
        "        Verifies that `convert_entry_to_stix` returns `None` when the RSS entry does\n",
        "        not contain a valid `published` field.\n",
        "\n",
        "        Example:\n",
        "            Entry:\n",
        "            - Title: \"Test Entry Without Date\"\n",
        "            - Summary: \"No date provided.\"\n",
        "            - Published: (missing)\n",
        "\n",
        "        Assertions:\n",
        "            - The returned STIX object is `None`.\n",
        "        \"\"\"\n",
        "        entry = {\n",
        "            \"title\": \"Test Entry Without Date\",\n",
        "            \"summary\": \"No date provided.\",\n",
        "        }\n",
        "        stix_object = convert_entry_to_stix(entry)\n",
        "        self.assertIsNone(stix_object)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ],
      "metadata": {
        "id": "kseFDY6jJKf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# requirements.txt\n",
        "\n",
        "feedparser==6.0.10      # For parsing RSS feeds\n",
        "stix2==3.0.0           # For creating STIX-compliant objects\n",
        "requests==2.31.0       # For making HTTP requests to fetch RSS feed content\n",
        "unittest-xml-reporting==3.2.0  # For enhanced unittest reporting (optional)\n"
      ],
      "metadata": {
        "id": "uSI-yZxHLOiL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
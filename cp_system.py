Given your CatBoost model and beta calibration setup, the residuals can be computed using the true labels and the calibrated probabilities generated by the beta calibration step. Here's how you can calculate the residuals:

---

### **Step 1: Ensure One-Hot Encoding of True Labels**
Since residuals are computed as the difference between true labels (one-hot encoded) and probabilities, you need to one-hot encode your true labels (`y_test`).

```python
from sklearn.preprocessing import LabelBinarizer
import numpy as np

# One-hot encode the true labels
lb = LabelBinarizer()
y_test_onehot = lb.fit_transform(y_test)

# If binary classification, ensure it has two columns
if y_test_onehot.shape[1] == 1:
    y_test_onehot = np.hstack([1 - y_test_onehot, y_test_onehot])
```

---

### **Step 2: Generate Calibrated Probabilities**
You already have the calibrated probabilities (`y_test_pred_calib`), which should have the same shape as the one-hot encoded true labels.

---

### **Step 3: Compute Residuals**
Residuals are computed as:

\[
\text{residuals} = \text{y\_test\_onehot} - \text{y\_test\_pred\_calib}
\]

Hereâ€™s the code:

```python
# Compute residuals for conformal prediction
residuals_test = y_test_onehot - y_test_pred_calib

# Print shape and some sample residuals
print("Residuals shape:", residuals_test.shape)
print("Sample residuals:\n", residuals_test[:5])
```

---

### **Step 4: Store Residuals for Conformal Predictive System**
Residuals can now be stored or used for conformal prediction.

```python
# Save residuals for later use
np.save("residuals_test.npy", residuals_test)
```

---

### **Important Notes:**
- If your `beta_calibrator` is generating probabilities for only one class (e.g., positive class in binary classification), you need to convert it to two-class probabilities by taking \([1 - \text{y\_test\_pred\_calib}, \text{y\_test\_pred\_calib}]\) for each sample.
- The residuals should align with the number of classes in your classification task.

Would you like assistance integrating this into your next step (e.g., fitting the conformal predictive system)?

To implement a **Conformal Predictive System (CPS)** based on the residuals computed from your beta-calibrated probabilities, here is the complete pipeline you can use:

### Required Libraries and Setup

First, ensure you have the necessary libraries installed:

```bash
pip install crepes
```

Import the required packages:

```python
import numpy as np
import pandas as pd
from crepes import ConformalPredictiveSystem
from crepes.extras import binning
import matplotlib.pyplot as plt
```

---

### **Step 1: Compute Residuals**
You have already computed the residuals for your calibration dataset (`residuals_cal`) and test dataset (`residuals_test`). If not:

```python
# Assuming you already have calibrated probabilities `y_pred_calib`
residuals_cal = y_cal - y_pred_calib  # Calibration residuals
residuals_test = y_test - y_test_pred_calib  # Test residuals
```

---

### **Step 2: Fit Standard and Normalized CPS**
Fit the standard and normalized CPS using the calibration residuals. Optionally, include difficulty estimates (`sigmas_cal_var`):

```python
# Standard CPS
cps_std = ConformalPredictiveSystem().fit(residuals_cal)

# Normalized CPS
sigmas_cal_var = np.std(residuals_cal, axis=0)  # Difficulty estimates (optional)
cps_norm = ConformalPredictiveSystem().fit(residuals_cal, sigmas=sigmas_cal_var)
```

---

### **Step 3: Fit Mondrian CPS**
To fit Mondrian CPS, create bins for the predictions. This groups data into categories for localized conformal prediction:

```python
# Create bins for calibration set
bins_cal, bin_thresholds = binning(y_pred_calib, bins=5)

# Mondrian CPS (Standard)
cps_mond_std = ConformalPredictiveSystem().fit(residuals_cal, bins=bins_cal)

# Mondrian CPS (Normalized)
cps_mond_norm = ConformalPredictiveSystem().fit(
    residuals_cal, sigmas=sigmas_cal_var, bins=bins_cal
)
```

---

### **Step 4: Assign Bins to Test Data**
For the test set, assign bins based on the same thresholds used during calibration:

```python
bins_test = binning(y_test_pred_calib, bins=bin_thresholds)
```

---

### **Step 5: Make Predictions with CPS**
You can now make predictions using CPS for your test data. These include p-values, thresholds, and prediction intervals.

#### Predict P-Values
```python
p_values = cps_mond_norm.predict(
    y_test_pred_calib,
    sigmas=np.std(residuals_test, axis=0),  # Difficulty estimates for test set
    bins=bins_test,
    y=y_test  # True labels for the test set
)
print("P-Values:\n", p_values)
```

#### Predict Thresholds
```python
thresholds = cps_mond_norm.predict(
    y_test_pred_calib,
    sigmas=np.std(residuals_test, axis=0),
    bins=bins_test,
    higher_percentiles=50  # For median thresholds
)
print("Thresholds:\n", thresholds)
```

#### Predict Full Prediction Intervals
Specify percentiles to extract prediction intervals (e.g., 95% confidence level):

```python
intervals = cps_mond_norm.predict(
    y_test_pred_calib,
    sigmas=np.std(residuals_test, axis=0),
    bins=bins_test,
    lower_percentiles=2.5,
    higher_percentiles=97.5
)
print("Prediction Intervals:\n", intervals)
```

---

### **Step 6: Evaluate CPS**
Evaluate CPS using coverage and size of prediction intervals:

```python
coverage = np.mean([
    1 if y_test[i] >= intervals[i, 0] and y_test[i] <= intervals[i, 1] else 0
    for i in range(len(y_test))
])
mean_interval_size = np.mean(intervals[:, 1] - intervals[:, 0])

print(f"Coverage: {coverage:.3f}")
print(f"Mean Interval Size: {mean_interval_size:.3f}")
```

---

### **Step 7: Visualize Prediction Intervals**
Plot the prediction intervals:

```python
plt.figure(figsize=(8, 6))
plt.title("Prediction Intervals")
plt.scatter(range(len(y_test)), y_test, color="blue", label="True Labels", alpha=0.6)
plt.scatter(range(len(y_test)), y_test_pred_calib, color="orange", label="Predicted Labels", alpha=0.6)
plt.vlines(range(len(y_test)), intervals[:, 0], intervals[:, 1], color="gray", alpha=0.3, label="Prediction Interval")
plt.legend()
plt.xlabel("Sample Index")
plt.ylabel("Values")
plt.show()
```

---

### **Step 8: Analyze Results**
Compare the coverage and size of intervals for various CPS methods:

```python
methods = {
    "Standard CPS": cps_std,
    "Normalized CPS": cps_norm,
    "Mondrian CPS (Standard)": cps_mond_std,
    "Mondrian CPS (Normalized)": cps_mond_norm,
}

for name, cps in methods.items():
    intervals = cps.predict(
        y_test_pred_calib,
        sigmas=np.std(residuals_test, axis=0),
        bins=bins_test,
        lower_percentiles=2.5,
        higher_percentiles=97.5
    )
    coverage = np.mean([
        1 if y_test[i] >= intervals[i, 0] and y_test[i] <= intervals[i, 1] else 0
        for i in range(len(y_test))
    ])
    mean_size = np.mean(intervals[:, 1] - intervals[:, 0])
    print(f"{name} - Coverage: {coverage:.3f}, Mean Size: {mean_size:.3f}")
```

---

This code sets up standard, normalized, and Mondrian CPS, assigns bins to test data, generates predictions, evaluates performance, and visualizes results. You can adapt it further based on your specific requirements.

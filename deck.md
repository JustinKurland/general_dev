Slide 4:

1. **Capacity Created**:  
   "With CRISS fully integrated into PATH Alert XDR, we’re reducing the manual effort required to classify and assess incidents. This streamlines our operations and frees up valuable team time."

2. **Shorter Elapsed Time/Cycle Time Reduction**:  
   "The flagging mechanism ensures incidents that need extra attention are surfaced quickly, helping us prioritize and resolve them faster, which keeps things moving smoothly."

3. **Quality**:  
   "By flagging likely misclassifications, we’re improving the overall accuracy of the system. This means fewer errors and a more standardized process."

4. **Financial**:  
   "Running the model weekly and targeting specific incidents helps us focus resources efficiently, reducing unnecessary costs and boosting overall operational efficiency."

5. **Behavioral/Human**:  
   "For our teams, this system provides clear, actionable outputs, building confidence in their decisions and creating a smoother, more satisfying workflow."

     
Slide 5:

This slide provides an overview of the system architecture, outlining the end-to-end process that underpins our machine learning model, from data collection to monitoring in production. While I'll be focusing on the stages highlighted in the boxes, the other steps are essential and further details about them can be found in the deck or on the Confluence page.

We start with **Data Collection and Preprocessing**, where we take historical incident data from ServiceNow, clean and standardize it to ensure consistency and usability.

Moving to **Feature Engineering**, this is where we create new features that enhance the model's ability to identify patterns and perform accurately.

For **Model Training and Validation**, we use advanced techniques like nested cross-validation and hyperparameter tuning with Optuna to ensure the model is robust and optimized for performance.

**Model Selection** is key—we chose the CatBoost algorithm because it's well-suited for the type of data we're working with, based on both empirical results and dataset characteristics.

Once trained, we apply **Post-Processing** using beta calibration. This step ensures that the probabilities generated by the model are well-calibrated, providing us with better uncertainty quantification.

Finally, **Monitoring** is crucial when the model is in production. It helps us ensure that the model continues to perform as expected over time by tracking feature inputs and outputs, as well as performance metrics.

These stages collectively build a pipeline that’s not only accurate but also reliable and ready for deployment into real-world operations.


Slide 6: 

"This slide highlights how our data preprocessing pipeline transforms raw, inconsistent data into a clean, standardized format suitable for machine learning modeling. As an example, features like 'u_impacted_regionnew' were reduced from 40 unique values to just 4 standardized categories, enabling better aggregation and analysis. Similarly, other features like 'u_impacted_countrynew' and 'u_impacted_citynew' saw significant reductions in unique values, simplifying the data without losing critical insights.

The preprocessing is powered by a Python data processing pipeline module, which performs these transformations in seconds. This not only ensures the model receives high-quality, consistent data but also enables us to capture and analyze emerging trends, such as geographic shifts in incidents, with greater accuracy and efficiency.


Slide 7:

This slide focuses on the extensive cleaning of unstructured text fields, such as descriptions, summaries, and work notes, all of which originally contained extensive embedded HTML due to how ServiceNow stores data. Using a range of preprocessing techniques, we’ve transformed these fields into clean, human-readable formats. 

This transformation is essential for our current classification model, ensuring the data is both structured and usable. Additionally, this cleaned text lays the foundation for future natural language processing efforts in 2025, such as identifying recurring or emerging themes in potential incidents. 

Beyond modeling, this cleaning process has operational benefits—it reduces the time needed to generate concise, accurate summaries for SIRs, ultimately supporting faster and clearer communication."


Slide 8:

This slide illustrates how we engineered features to create meaningful signals for the model. 

First, we applied **target encoding** to the `u_risk_level` feature, which is the target variable for this binary classification model. This encoding translates the target labels into numerical values, a necessary step for supervised learning, as it allows the model to train and evaluate performance effectively.

Next, we computed **time deltas**, such as the difference between the incident's opening time and its last update. This provides the model with valuable temporal information that can help identify patterns or anomalies related to the incident's lifecycle.

Finally, we used **one-hot encoding** for low-cardinality categorical features. This is simply a method of converting values into binary indicators, such as 0s and 1s, so the model can process and interpret them correctly. For example, region data like 'APAC' or 'EMEA' is transformed into a format the model can use to make predictions.


Slide 9:

Ordered Target Encoding: For high-cardinality categorical features (those with more than 5 categories), we applied ordered target encoding. This technique creates a numerical representation of each category based on the average target value for that category, providing a powerful signal for the model while mitigating potential overfitting. This approach is particularly useful when dealing with categorical data, a common challenge for data scientists.


Slide 10: 

This slide shows how we handle the challenge of using unstructured text data, such as descriptions, summaries, and notes. We convert this text into numerical vectors called embeddings using a specialized model.

For example, the phrase "GS Users using Box for Collaboration" is transformed into a series of numbers like 0.6, 0.5, 0.1, and so on. These numbers capture the meaning and context of the text, allowing the model to understand and use this information during training and prediction.

In essence, embeddings turn raw text into a format that the machine learning model can effectively process and learn from.


Slide 11: 

This slide presents the performance of our machine learning model on a held-out test dataset, providing a realistic estimate of its performance in production.

As you can see, the model achieves a high accuracy of 99.81%, meaning it correctly predicts whether an incident is a risk or not in the vast majority of cases.

Precision, the ratio of true risks correctly predicted to the total predicted risks, is at 88.5%. This indicates that when the model predicts a risk, it is highly likely to be an actual risk.

Recall, the ratio of true risks predicted to the total number of actual risks, is at 92.6%. This means the model captures a large proportion of the true risks present in the data.

It's important to note that we conducted a manual review of the false positives and false negatives with Matt and Polina. We found that some of these instances were borderline cases or situations where the risk level was subjective, suggesting that the model is performing quite well in discerning actual risk.

Furthermore, this model significantly outperforms a naive model, demonstrating its ability to capture meaningful signals and make accurate predictions.

Overall, these results demonstrate the effectiveness of our machine learning model in identifying potential risks within incidents.


Slide 12:

This timeline illustrates the journey of our machine learning model from data collection to deployment.

2023-2024: Data collection period.
End of 2024: Model development completed.
Jan. 2025: CRISS Deployment.
Feb. 2025: Automate Short Descriptions (PATH Alert XDR)
March 2025: Trend/Topic Model leveraging case notes (PATH Alert XDR)
Ongoing: Continuous monitoring and periodic retraining of the model.
Looking ahead, this project opens up exciting possibilities:

Capacity Created: Automating tasks like short description generation will free up valuable analyst time for more strategic activities.
Shorter Elapsed Time/Cycle Time Reduction: Faster risk identification and prioritization will lead to quicker incident resolution, improving customer satisfaction.
Quality: The model's consistent and accurate risk predictions will standardize the incident classification process and reduce errors.
Financial: Automating tasks and reducing manual effort will translate into significant cost savings over time.
Behavioral/Human: Analysts will gain confidence in their decisions and enjoy a smoother workflow with the support of the model's insights.
This model is not just a tool; it's a catalyst for transformation, enabling us to operate more efficiently, effectively, and confidently.
